@article{LADOROIGE2022112218,
	title = {Learning-based video motion magnification approach for vibration-based damage detection},
	journal = {Measurement},
	pages = {112218},
	year = {2022},
	issn = {0263-2241},
	doi = {10.1016/j.measurement.2022.112218},
	author = {Ricard Lado-Roigé and Josep Font-Moré and Marco A. Pérez},
	keywords = {Computer vision, Convolutional Neural Network, Damage identification, Deep learning, Frequency response functions, Natural frequency, Operating deflection shape, Structural assessment, Structural health monitoring, Vibration testing},
}
@article{lado2022_STB-VMM,
	title={STB-VMM: Swin Transformer Based Video Motion Magnification},
	doi = {https://doi.org/10.48550/arXiv.2302.10001},
	author={Lado-Roigé, Ricard and P{\'{e}}rez, Marco A.},
	journal={Knowledge-Based Systems (Submitted July 12, 2022)},
	year={2022},
	note = { (Under review) }
}
@article{oh_learning-based_2018,
	abstract = {Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract motion representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works.},
	author = {Oh, Tae-Hyun and Jaroensri, Ronnachai and Kim, Changil and Elgharib, Mohamed and Durand, Frédo and Freeman, William T. and Matusik, Wojciech},
	doi = {10.48550/arXiv.1804.02684},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	language = {en},
	title = {Learning-based Video Motion Magnification},
	year = {2018}
}

@article{MOLINAVIEDMA2018245,
	title = {High frequency mode shapes characterisation using Digital Image Correlation and phase-based motion magnification},
	journal = {Mech. Syst. Sig. Process.},
	volume = {102},
	pages = {245-261},
	year = {2018},
	issn = {0888-3270},
	doi = {10.1016/j.ymssp.2017.09.019},
	author = {A.J. Molina-Viedma and L. Felipe-Sesé and E. López-Alba and F. Díaz},
	keywords = {High Speed Digital Image Correlation, Mode shapes, Operational Deflection Shapes, Motion magnification},
	abstract = {High speed video cameras provide valuable information in dynamic events. Mechanical characterisation has been improved by the interpretation of the behaviour in slow-motion visualisations. In modal analysis, videos contribute to the evaluation of mode shapes but, generally, the motion is too subtle to be interpreted. In latest years, image treatment algorithms have been developed to generate a magnified version of the motion that could be interpreted by naked eye. Nevertheless, optical techniques such as Digital Image Correlation (DIC) are able to provide quantitative information of the motion with higher sensitivity than naked eye. For vibration analysis, mode shapes characterisation is one of the most interesting DIC performances. Full-field measurements provide higher spatial density than classical instrumentations or Scanning Laser Doppler Vibrometry. However, the accurateness of DIC is reduced at high frequencies asa consequence of the low displacements and hence it is habitually employed in low frequency spectra. In the current work, the combination of DIC and motion magnification is explored in order to provide numerical information in magnified videos and perform DIC mode shapes characterisation at unprecedented high frequencies through increasing the amplitude of displacements.}
}

@article{EITNER2021106995,
	title = {Effect of broad-band phase-based motion magnification on modal parameter estimation},
	journal = {Mechanical Systems and Signal Processing},
	volume = {146},
	pages = {106995},
	year = {2021},
	issn = {0888-3270},
	doi = {10.1016/j.ymssp.2020.106995},
	author = {Marc Eitner and Benjamin Miller and Jayant Sirohi and Charles Tinney},
	keywords = {Operational modal analysis, Phase-based motion magnification, Complexity pursuit, Blind source separation, Optical vibration measurement, Point tracking},
	abstract = {In this paper, broad-band phase-based motion magnification (BPMM) is used to improve the modal parameter estimation from high-speed video of a structure undergoing low amplitude vibration. The authors use a novel application of the phase-based motion magnification technique to obtain improved results in the presence of image noise. A numerical simulation is performed to demonstrate and quantify the effect of broad-band motion magnification. A clear correlation is found between the magnification factor and the error in the motion obtained from 2-dimensional point tracking (2DPT). In a laboratory experiment, operational modal analysis is performed on a metallic nozzle with a 5.3 in exit diameter and a constant wall thickness of 0.03 inches. Fluorescent markers are painted on the nozzle lip. The nozzle is attached to a test stand and excited by pressurized air. The induced vibrations are captured by a single high-speed camera, which takes images of the nozzle lip. Using a 2DPT algorithm, the displacement history of the markers in the nozzle exit plane is extracted. As is typical for small and stiff structures, the resonant frequencies are quite high and the resulting vibration amplitudes are fairly low. This leads to low signal-to-noise ratio in the higher frequencies, which makes the following operational modal analysis much harder. By using BPMM as a preprocessor, the authors demonstrate that the outcome of modal analysis is greatly improved. Comparison with Finite Element analysis shows that the mode shapes agree much better with the experimental results when motion magnification is used. The modal parameters of the first six modes are found in the frequency range 0–1400 Hz using the new methodology, whereas without motion magnification only the first five modes are found.}
}

@article{10.1145/2601097.2601119,
	author = {Davis, Abe and Rubinstein, Michael and Wadhwa, Neal and Mysore, Gautham J. and Durand, Fr\'{e}do and Freeman, William T.},
	title = {The Visual Microphone: Passive Recovery of Sound from Video},
	year = {2014},
	issue_date = {July 2014},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {33},
	number = {4},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2601097.2601119},
	doi = {10.1145/2601097.2601119},
	abstract = {When sound hits an object, it causes small vibrations of the object's surface. We show how, using only high-speed video of the object, we can extract those minute vibrations and partially recover the sound that produced them, allowing us to turn everyday objects---a glass of water, a potted plant, a box of tissues, or a bag of chips---into visual microphones. We recover sounds from high-speed footage of a variety of objects with different properties, and use both real and simulated data to examine some of the factors that affect our ability to visually recover sound. We evaluate the quality of recovered sounds using intelligibility and SNR metrics and provide input and recovered audio samples for direct comparison. We also explore how to leverage the rolling shutter in regular consumer cameras to recover audio from standard frame-rate videos, and use the spatial resolution of our method to visualize how sound-related vibrations vary over an object's surface, which we can use to recover the vibration modes of an object.},
	journal = {ACM Trans. Graph.},
	month = {jul},
	articleno = {79},
	numpages = {10},
	keywords = {sound from video, remote sound acquisition, visual acoustics}
}

@article{frangopol_effects_1987,
	abstract = {Analytical investigations into the effects of damage and redundancy on the reliability of structural systems are reported. These investigations are based on a definition of structural redundancy including both system reliability and damage assessment concepts. Truss systems and bridges analyzed for different damage scenarios are used to develop the theoretical concepts and to illustrate their practical applications.},
	author = {Frangopol, Dan M. and Curley, James P.},
	copyright = {Copyright © 1987 ASCE},
	doi = {10.1061/(ASCE)0733-9445(1987)113:7(1533)},
	issn = {0733-9445},
	journal = {J. Struct. Eng.},
	keywords = {Bridges, Damage assessment, Redundancy, Structural behavior, System reliability, Trusses},
	language = {EN},
	month = {July},
	number = {7},
	pages = {1533--1549},
	title = {Effects of Damage and Redundancy on Structural Reliability},
	volume = {113},
	year = {1987}
}

@article{cosenza_damage_2000,
	abstract = {Abstract This article reviews the ground motion parameters than can be assumed as structural and non-structural damage measures. Measures of seismic damage potential based on ground motion records are first described, followed by a discussion of the damage measures relating to simple (linear) and more complex (non-linear) structural responses. The second section reviews the measures of damage phenomena which govern structural degradation and/or collapse, including experimental results and analytical models. The relationship between earthquake characteristics and type and level of damage on the seismic response of structures is examined, and data from different well-known destructive earthquakes are given.},
	author = {Cosenza, Edoardo and Manfredi, Gaetano},
	doi = {10.1002/(SICI)1528-2716(200001/03)2:1<50::AID-PSE7>3.0.CO;2-S},
	issn = {1365-0556},
	journal = {Prog. in Struct. Eng.   Mater.s},
	month = {January},
	number = {1},
	pages = {50--59},
	title = {Damage indices and damage measures},
	volume = {2},
	year = {2000}
}

@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{ViT,
	doi = {10.48550/ARXIV.2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	publisher = {arXiv},
	year = {2020},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Vaswani2017AttentionIA,
	doi = {10.48550/ARXIV.1706.03762},	
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Attention Is All You Need},
	publisher = {arXiv},
	year = {2017},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SWIN,
	doi = {10.48550/ARXIV.2103.14030},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
	publisher = {arXiv},
	year = {2021},
	copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{10.1007/978-3-030-59716-0_34,
	author="Janatka, Mirek
	and Marcus, Hani J.
	and Dorward, Neil L.
	and Stoyanov, Danail",
	title="Surgical Video Motion Magnification with Suppression of Instrument Artefacts",
	booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2020",
	year="2020",
	publisher="Springer International Publishing",
	address="Cham",
	pages="353--363",
	abstract="Video motion magnification can make blood vessels in surgical video more apparent by exaggerating their pulsatile motion and could prevent inadvertent damage and bleeding due to their increased prominence. It could also indicate the success of restricting blood supply to an organ when using a vessel clamp. However, the direct application to surgical video could result in aberration artefacts caused by its sensitivity to residual motion from the surgical instruments and would impede its practical usage in the operating theatre. By storing the previously obtained jerk filter response of each spatial component of each image frame - both prior to surgical instrument introduction and adhering to a Eulerian frame of reference - it is possible to prevent such aberrations from occurring. The comparison of the current readings to the prior readings of a single cardiac cycle at the corresponding cycle point, are used to determine if motion magnification should be active for each spatial component of the surgical video at that given point in time. In this paper, we demonstrate this technique and incorporate a scaling variable to loosen the effect which accounts for variabilities and misalignments in the temporal domain. We present promising results on endoscopic transnasal transsphenoidal pituitary surgery with a quantitative comparison to recent methods using Structural Similarity (SSIM), as well as qualitative analysis by comparing spatio-temporal cross sections of the videos and individual frames.",
	isbn="978-3-030-59716-0",
	doi={10.48550/arXiv.2009.07432}
}
